{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6866b9f7-b8cf-4ad1-9f5f-21386773a13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¥ Loading dataset from Hugging Face...\n",
      "\n",
      "ğŸ”  Tokenizing text...\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "# âœ… EMOTION DETECTION MODEL (GloVe + BiLSTM)\n",
    "# Dataset: dair-ai/emotion (Hugging Face)\n",
    "# Model: BiLSTM + GloVe (100D)\n",
    "###########################################\n",
    "\n",
    "# âœ… STEP 0: Install if not already\n",
    "# pip install datasets tensorflow numpy matplotlib\n",
    "\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "# âœ… STEP 1: Load Dataset\n",
    "print(\"\\nğŸ“¥ Loading dataset from Hugging Face...\")\n",
    "dataset = load_dataset(\"dair-ai/emotion\")\n",
    "train_ds, val_ds, test_ds = dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"]\n",
    "label_names = dataset[\"train\"].features[\"label\"].names\n",
    "num_classes = len(label_names)\n",
    "\n",
    "# âœ… STEP 2: Tokenizer on all text (train + val + test)\n",
    "print(\"\\nğŸ”  Tokenizing text...\")\n",
    "all_texts = [ex[\"text\"] for ex in train_ds] + \\\n",
    "            [ex[\"text\"] for ex in val_ds] + \\\n",
    "            [ex[\"text\"] for ex in test_ds]\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\", num_words=10000)\n",
    "tokenizer.fit_on_texts(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6187a1c0-bdbe-476e-ae88-2719f8ff16ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Prepare sequences\n",
    "max_len = 50\n",
    "def to_seq(ds): return pad_sequences(tokenizer.texts_to_sequences([ex[\"text\"] for ex in ds]), maxlen=max_len)\n",
    "\n",
    "x_train = to_seq(train_ds)\n",
    "x_val = to_seq(val_ds)\n",
    "x_test = to_seq(test_ds)\n",
    "\n",
    "y_train = np.array([ex[\"label\"] for ex in train_ds])\n",
    "y_val = np.array([ex[\"label\"] for ex in val_ds])\n",
    "y_test = np.array([ex[\"label\"] for ex in test_ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6c47183e-fdd5-459c-b649-63d27186fb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“š Loading GloVe embeddings...\n"
     ]
    }
   ],
   "source": [
    "# âœ… STEP 3: Load GloVe Embeddings\n",
    "print(\"\\nğŸ“š Loading GloVe embeddings...\")\n",
    "embedding_dim = 100\n",
    "vocab_size = 10000\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "with open(\"glove.6B.100d.txt\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0]\n",
    "        vector = np.asarray(parts[1:], dtype='float32')\n",
    "        idx = tokenizer.word_index.get(word)\n",
    "        if idx is not None and idx < vocab_size:\n",
    "            embedding_matrix[idx] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "866b3b8f-94eb-4c3c-b192-390b02875d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§  Building model...\n"
     ]
    }
   ],
   "source": [
    "# âœ… STEP 4: Build BiLSTM Model\n",
    "print(\"\\nğŸ§  Building model...\")\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size,\n",
    "              output_dim=embedding_dim,\n",
    "              weights=[embedding_matrix],\n",
    "              input_length=max_len,\n",
    "              trainable=True),\n",
    "    Bidirectional(LSTM(128, return_sequences=True)),\n",
    "    Dropout(0.3),\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "455eefbd-68e9-4255-a13b-46ef4573e04b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                         </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape                </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              â”‚ ?                           â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      â”‚ ?                           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 â”‚ ?                           â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      â”‚ ?                           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 â”‚ ?                           â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     â”‚ ?                           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 â”‚ ?                           â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     â”‚ ?                           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding_9 (\u001b[38;5;33mEmbedding\u001b[0m)              â”‚ ?                           â”‚       \u001b[38;5;34m1,000,000\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_3 (\u001b[38;5;33mBidirectional\u001b[0m)      â”‚ ?                           â”‚     \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)                 â”‚ ?                           â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_4 (\u001b[38;5;33mBidirectional\u001b[0m)      â”‚ ?                           â”‚     \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)                 â”‚ ?                           â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_14 (\u001b[38;5;33mDense\u001b[0m)                     â”‚ ?                           â”‚     \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)                 â”‚ ?                           â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_15 (\u001b[38;5;33mDense\u001b[0m)                     â”‚ ?                           â”‚     \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> (3.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,000,000\u001b[0m (3.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> (3.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,000,000\u001b[0m (3.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# âœ… STEP 5: Compile Model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bb54dd78-463d-496d-8602-0e99f782f59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Training model...\n",
      "Epoch 1/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 359ms/step - accuracy: 0.2741 - loss: 1.7007 - val_accuracy: 0.3955 - val_loss: 1.5536\n",
      "Epoch 2/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 336ms/step - accuracy: 0.4016 - loss: 1.5461 - val_accuracy: 0.5110 - val_loss: 1.3567\n",
      "Epoch 3/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 341ms/step - accuracy: 0.4996 - loss: 1.3762 - val_accuracy: 0.5240 - val_loss: 1.2813\n",
      "Epoch 4/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 337ms/step - accuracy: 0.5243 - loss: 1.2988 - val_accuracy: 0.5435 - val_loss: 1.2055\n",
      "Epoch 5/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 340ms/step - accuracy: 0.5542 - loss: 1.2177 - val_accuracy: 0.5540 - val_loss: 1.1505\n",
      "Epoch 6/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 338ms/step - accuracy: 0.5579 - loss: 1.1741 - val_accuracy: 0.5610 - val_loss: 1.1043\n",
      "Epoch 7/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 339ms/step - accuracy: 0.5774 - loss: 1.1092 - val_accuracy: 0.6070 - val_loss: 1.0345\n",
      "Epoch 8/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 343ms/step - accuracy: 0.6148 - loss: 1.0346 - val_accuracy: 0.6345 - val_loss: 0.9691\n",
      "Epoch 9/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 347ms/step - accuracy: 0.6463 - loss: 0.9469 - val_accuracy: 0.6655 - val_loss: 0.9018\n",
      "Epoch 10/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 319ms/step - accuracy: 0.6870 - loss: 0.8572 - val_accuracy: 0.6900 - val_loss: 0.8472\n",
      "Epoch 11/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 306ms/step - accuracy: 0.7163 - loss: 0.7885 - val_accuracy: 0.7215 - val_loss: 0.7653\n",
      "Epoch 12/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 343ms/step - accuracy: 0.7508 - loss: 0.7054 - val_accuracy: 0.7515 - val_loss: 0.7053\n",
      "Epoch 13/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 331ms/step - accuracy: 0.7756 - loss: 0.6492 - val_accuracy: 0.7625 - val_loss: 0.6715\n",
      "Epoch 14/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 309ms/step - accuracy: 0.8032 - loss: 0.5803 - val_accuracy: 0.7835 - val_loss: 0.6267\n",
      "Epoch 15/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 323ms/step - accuracy: 0.8181 - loss: 0.5349 - val_accuracy: 0.7900 - val_loss: 0.6042\n",
      "Epoch 16/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 329ms/step - accuracy: 0.8346 - loss: 0.4984 - val_accuracy: 0.7955 - val_loss: 0.5817\n",
      "Epoch 17/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 333ms/step - accuracy: 0.8520 - loss: 0.4435 - val_accuracy: 0.8005 - val_loss: 0.6140\n",
      "Epoch 18/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 339ms/step - accuracy: 0.8593 - loss: 0.4201 - val_accuracy: 0.8350 - val_loss: 0.5066\n",
      "Epoch 19/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 332ms/step - accuracy: 0.8712 - loss: 0.3752 - val_accuracy: 0.8365 - val_loss: 0.5164\n",
      "Epoch 20/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 331ms/step - accuracy: 0.8795 - loss: 0.3649 - val_accuracy: 0.8425 - val_loss: 0.4875\n",
      "Epoch 21/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 338ms/step - accuracy: 0.8929 - loss: 0.3327 - val_accuracy: 0.8445 - val_loss: 0.4721\n",
      "Epoch 22/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 320ms/step - accuracy: 0.8986 - loss: 0.3037 - val_accuracy: 0.8360 - val_loss: 0.4952\n",
      "Epoch 23/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 315ms/step - accuracy: 0.9083 - loss: 0.2834 - val_accuracy: 0.8620 - val_loss: 0.4431\n",
      "Epoch 24/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 342ms/step - accuracy: 0.9112 - loss: 0.2761 - val_accuracy: 0.8545 - val_loss: 0.4555\n",
      "Epoch 25/25\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 337ms/step - accuracy: 0.9148 - loss: 0.2512 - val_accuracy: 0.8675 - val_loss: 0.4348\n"
     ]
    }
   ],
   "source": [
    "# âœ… STEP 6: Train Model\n",
    "print(\"\\nğŸš€ Training model...\")\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "history = model.fit(x_train, y_train,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    epochs=25,\n",
    "                    batch_size=128,\n",
    "                    callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c73463bb-54ec-4be0-aafc-a273c74252e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Evaluating...\n",
      "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 59ms/step - accuracy: 0.8532 - loss: 0.4988\n",
      "Test Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "# âœ… STEP 7: Evaluate Model\n",
    "print(\"\\nğŸ“Š Evaluating...\")\n",
    "loss, acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "62c703e0-9afa-41f1-afd3-727831e2777c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ Saving model & tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# âœ… STEP 8: Save Artifacts\n",
    "print(\"\\nğŸ’¾ Saving model & tokenizer...\")\n",
    "model.save(\"emotion_model.h5\")\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "with open(\"label_names.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_names, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cba82d50-b373-49f8-b73b-8810ddf4d694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Original Text: I am so happy and excited today!\n",
      "ğŸ”§ Corrected Text: I am so happy and excited today\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "ğŸ­ Predicted Emotion: joy (Confidence: 1.00)\n",
      "\n",
      "ğŸ“ Original Text: I hate everything about this terible day.\n",
      "ğŸ”§ Corrected Text: I hate everything about this terrible day\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "ğŸ­ Predicted Emotion: sadness (Confidence: 0.89)\n",
      "\n",
      "ğŸ“ Original Text: It is a grate dissapointment.\n",
      "ğŸ”§ Corrected Text: It is a gate disappointment\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
      "ğŸ­ Predicted Emotion: sadness (Confidence: 0.33)\n",
      "\n",
      "ğŸ“ Original Text: Iâ€™m fellng so hopless nd alone\n",
      "ğŸ”§ Corrected Text: im felling so hopeless nd alone\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "ğŸ­ Predicted Emotion: sadness (Confidence: 0.99)\n"
     ]
    }
   ],
   "source": [
    "from rapidfuzz import fuzz\n",
    "from rapidfuzz import process\n",
    "\n",
    "# âœ… Create a set of all known words from tokenizer\n",
    "known_words = set(tokenizer.word_index.keys())\n",
    "\n",
    "# âœ… Function to correct each word using fuzzy matching\n",
    "def correct_spelling(text, threshold=80):\n",
    "    corrected_words = []\n",
    "    for word in text.split():\n",
    "        if word.lower() in known_words:\n",
    "            corrected_words.append(word)\n",
    "        else:\n",
    "            # Use fuzzy matching to find closest word in vocab\n",
    "            best_match = process.extractOne(word.lower(), known_words, scorer=fuzz.ratio)\n",
    "            if best_match and best_match[1] >= threshold:\n",
    "                corrected_words.append(best_match[0])\n",
    "            else:\n",
    "                corrected_words.append(word)  # Keep as is if no good match\n",
    "    return \" \".join(corrected_words)\n",
    "\n",
    "# âœ… Enhanced Emotion Prediction with Fuzzy Correction\n",
    "def predict_emotion(text):\n",
    "    print(f\"\\nğŸ“ Original Text: {text}\")\n",
    "\n",
    "    corrected_text = correct_spelling(text)\n",
    "    print(f\"ğŸ”§ Corrected Text: {corrected_text}\")\n",
    "\n",
    "    seq = tokenizer.texts_to_sequences([corrected_text])\n",
    "    padded = pad_sequences(seq, maxlen=max_len)\n",
    "\n",
    "    pred = model.predict(padded)[0]\n",
    "    label = label_names[np.argmax(pred)]\n",
    "\n",
    "    print(f\"ğŸ­ Predicted Emotion: {label} (Confidence: {np.max(pred):.2f})\")\n",
    "\n",
    "# âœ… Test Cases\n",
    "predict_emotion(\"I am so happy and excited today!\")\n",
    "predict_emotion(\"I hate everything about this terible day.\")\n",
    "predict_emotion(\"It is a grate dissapointment.\")\n",
    "predict_emotion(\"Iâ€™m fellng so hopless nd alone\")  # Test edge-case typos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb5d1c2-f242-48f6-ab51-0d461901f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | Step | Description                           |\n",
    "# | ---- | ------------------------------------- |\n",
    "# | 1    | Load & explore labeled tweets dataset |\n",
    "# | 2    | Tokenize words â†’ integers             |\n",
    "# | 3    | Load GloVe embeddings (100D)          |\n",
    "# | 4    | Build embedding matrix                |\n",
    "# | 5    | Create LSTM model using embeddings    |\n",
    "# | 6    | Train & evaluate                      |\n",
    "# | 7    | Predict emotion from any new text     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d7a9b9da-0d93-4306-a650-e4475b2bc663",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2473639437.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[57], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Step 1: Load Dataset\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#  Step 1: Load Dataset\n",
    "# dataset = load_dataset(\"dair-ai/emotion\")\n",
    "# train_ds, val_ds, test_ds = dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"]\n",
    "# We load a dataset of tweets labeled with 6 emotions:\n",
    "\n",
    "# 'sadness', 'joy', 'love', 'anger', 'fear', 'surprise'\n",
    "\n",
    "# It returns 3 parts: train_ds, val_ds, test_ds (for training, validation, and testing)\n",
    "\n",
    "# ğŸ“Œ Example sample:\n",
    "# {'text': \"i'm feeling quite sad and depressed\", 'label': 0}\n",
    "\n",
    "#     ğŸ”¹ Step 2: Explore the Dataset\n",
    "# labels = [label_names[ex[\"label\"]] for ex in train_ds]\n",
    "# Extracts the label names from all training samples.\n",
    "\n",
    "# Then uses Counter() + matplotlib to plot the distribution of emotions.\n",
    "\n",
    "# ğŸ“Š Helps you know how balanced the dataset is.\n",
    "\n",
    "#     ğŸ”¹ Step 3: Tokenize and Pad Texts\n",
    "# tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "# tokenizer.fit_on_texts(texts)\n",
    "# Creates a word-to-index mapping:\n",
    "# e.g., \"happy\" â†’ 57, \"love\" â†’ 89\n",
    "\n",
    "# <OOV> means \"out of vocabulary\" â€” used for unknown words\n",
    "\n",
    "# Then we convert text to sequences of numbers:\n",
    "# x_train_seq = tokenizer.texts_to_sequences(...)\n",
    "# x_train = pad_sequences(x_train_seq, maxlen=50)\n",
    "# Converts:\n",
    "# \"I am sad today\" â†’ [1, 6, 57, 22] â†’ [1, 6, 57, 22, 0, 0, ..., 0] (padded to 50)\n",
    "# This ensures all input texts are same length for training\n",
    "# Step 4: Load GloVe Word Embeddings\n",
    "# with open(\"glove.6B.100d.txt\") as f:\n",
    "#     glove_embeddings[word] = vector\n",
    "\n",
    "# Loads 400,000 English words, each mapped to a 100-dimensional vector\n",
    "\n",
    "# Example:\n",
    "# \"happy\" â†’ [0.12, -0.44, ..., 0.27]\n",
    "# These vectors capture meaning and relationships between words\n",
    "# Step 5: Build the Embedding Matrix\n",
    "# embedding_matrix = np.zeros((vocab_size, 100))\n",
    "# for word, i in tokenizer.word_index.items():\n",
    "#     embedding_matrix[i] = glove_embeddings.get(word)\n",
    "\n",
    "# Maps each word in our dataset to its GloVe vector\n",
    "\n",
    "# If GloVe has no vector, it stays as zeros\n",
    "\n",
    "# ğŸ“˜ Why? So our model uses semantic meaning of words from the start, instead of learning them from scratch.\n",
    "\n",
    "# ğŸ”¹ Step 6: Define the Model\n",
    "# model = Sequential([\n",
    "#     Embedding(..., weights=[embedding_matrix], trainable=False),\n",
    "#     LSTM(64),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dense(num_classes, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# | Layer             | Purpose                                                            |\n",
    "# | ----------------- | ------------------------------------------------------------------ |\n",
    "# | **Embedding**     | Converts word indices into their 100D vector meanings (from GloVe) |\n",
    "# | **LSTM(64)**      | Reads the word vectors as a sequence and remembers context         |\n",
    "# | **Dense(64)**     | Learns deep patterns from LSTM output                              |\n",
    "# | **Dense(output)** | Outputs 6 numbers = probability of each emotion (via softmax)      |\n",
    "\n",
    "\n",
    "# Step 7: Compile and Train the Model\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# history = model.fit(...)\n",
    "\n",
    "# sparse_categorical_crossentropy is used because your labels are integers (not one-hot)\n",
    "\n",
    "# Trains for 5 epochs\n",
    "\n",
    "# ğŸ”¹ Step 8: Evaluate the Model\n",
    "# test_loss, test_acc = model.evaluate(x_test, np.array(y_test))\n",
    "\n",
    "# ğŸ”¹ Step 9: Predict Emotion of New Text\n",
    "# def predict_emotion(text):\n",
    "#     seq = tokenizer.texts_to_sequences([text])\n",
    "#     padded = pad_sequences(seq, maxlen=max_len)\n",
    "#     pred = model.predict(padded)[0]\n",
    "#     label = label_names[np.argmax(pred)]\n",
    "\n",
    "#     Converts input text to token sequence\n",
    "\n",
    "# Pads it to max length (50)\n",
    "\n",
    "# Feeds it to the model\n",
    "\n",
    "# Gets output like:\n",
    "# [0.01, 0.03, 0.92, 0.01, 0.01, 0.02] â†’ 'joy'\n",
    "\n",
    "# ğŸ”® Final Output Example\n",
    "# predict_emotion(\"I am so happy and excited today!\")\n",
    "\n",
    "# ğŸ“ Text: I am so happy and excited today!\n",
    "# ğŸ­ Predicted Emotion: joy (Confidence: 0.93)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ca7557-15df-4c37-a7c6-e2d4d17cd1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | Layer No. | Layer Type              | Description                                                                              |\n",
    "# | --------- | ----------------------- | ---------------------------------------------------------------------------------------- |\n",
    "# | 1ï¸âƒ£       | **Embedding**           | Maps each word (by index) to a 100D GloVe vector.                                        |\n",
    "# | 2ï¸âƒ£       | **LSTM(64)**            | Processes the word vector sequence to capture **temporal context** (word order, memory). |\n",
    "# | 3ï¸âƒ£       | **Dense(64)**           | Fully connected layer to learn complex features from LSTM output.                        |\n",
    "# | 4ï¸âƒ£       | **Dense(num\\_classes)** | Final layer to predict probability for each emotion class using **softmax**.             |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
